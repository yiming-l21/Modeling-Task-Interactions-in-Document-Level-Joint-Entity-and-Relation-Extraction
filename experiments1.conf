task=re
# data related
dataset_name = "envredocred"
data_dir = "./data"
dataset_dir = "./data"
train_partition = "train_revised"
dev_partition = "dev_revised_env"
test_partition = "test_revised_env"
log_root = "./logs"
pretrained = "/home/ymliu/robustness_on_RE_models/bert-base-cased"
tokenizer = "/home/ymliu/robustness_on_RE_models/bert-base-cased"

# model related
model_type = "bert"
re_config_name = "re"
coref_config_name = "coref"
coref_model_suffix ="./models/coref/best_model.bin"
re_model_suffix = "./models/re/best_model.bin"
max_seq_len = 512
overlapping = 128
max_training_seg = 512
dropout_rate = 0.1
max_span_width = 30
span_w_boundary = true
span_w_tokens = true
use_span_width = true
use_span_type = true
pair_span_transform = true
pair_span_hidden_size = 2312 #论文中是2048，但是2048会报错不匹配
pair_with_similarity = true
use_antecedent_distance = true
use_speaker_indicator = false
coref_loss_coef = 1.0
fine_grained = true
coref_propagation = true
scalar_gate = true
num_re_labels = 97
max_re_labels = 10
entity_transform_size = 128
num_blocks = 4
use_entity_type = true
use_entity_distance = true
eval_batch_size = 24
allow_singletons = true

# training related
mention_heads = true
use_width_prior = true
approx_pruning = true
max_num_extracted_spans = 512
top_span_ratio = 0.4
max_top_antecedents = 50
use_antecedent_distance_prior = true
void_negative = true
use_amp = true
num_epochs = 72  # 对DocRED数据集
gradient_accumulation_steps = 1
batch_size = 16  # 调整批量大小，论文中是4，但是4太慢了
bert_learning_rate = 5e-5  # 根据论文调整
task_learning_rate = 2e-4  # 根据论文调整
bert_wd = 0.05
task_wd = 0.05
adam_eps = 1e-8
warmup_ratio = 0.1
mention_loss_coef = 0.5
max_grad_norm = 1.0
report_frequency = 10
eval_frequency = 2000