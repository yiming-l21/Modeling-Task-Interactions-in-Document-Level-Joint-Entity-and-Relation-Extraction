#data related
dataset_name="docred"
data_dir="./data"
dataset_dir="./data"
train_partition="train_annotated"
dev_partition="dev"
test_partition="test"
log_root="./logs"
pretrained="/home/ymliu/robustness_on_RE_models/bert-base-cased"
tokenizer="/home/ymliu/robustness_on_RE_models/bert-base-cased"
#model related
model_type="bert"
re_config_name="re"
coref_config_name="coref"
coref_model_suffix="./models/coref/best_model.bin"
re_model_suffix="./models/re/best_model.bin"
max_seq_len=512
overlapping=128
max_training_seg=512
dropout_rate=0.1
max_span_width=30
span_w_boundary=true
span_w_tokens=true
use_span_width=true
use_span_type=true
pair_span_transform=true
pair_span_hidden_size=2312
pair_with_similarity=true
use_antecedent_distance = true
use_speaker_indicator = false
coref_loss_coef = 1.0
fine_grained = true
coref_propagation = true
scalar_gate = true
num_re_labels=97
max_re_labels=10
entity_transform_size=128
num_blocks=4
use_entity_type=true
use_entity_distance=true
eval_batch_size=24
allow_singletons=true
#training related
mention_heads = true
use_width_prior = true
approx_pruning = true
max_num_extracted_spans=512
top_span_ratio=0.4
max_top_antecedents=50
use_antecedent_distance_prior=true
void_negative=true
use_amp=true
num_epochs=50
gradient_accumulation_steps=1
batch_size=16
bert_learning_rate = 5e-6  # 更低的学习率
task_learning_rate = 1e-5  # 更低的任务学习率
bert_wd = 0.05
task_wd = 0.05
adam_eps = 1e-8
warmup_ratio = 0.1
mention_loss_coef = 0.5
max_grad_norm = 1.0
report_frequency = 10
eval_frequency = 5